import pandas as pd
import openai
import traceback  # To print error messages if needed

# Define your key.
openai.api_key = "<API_KEY>"

# Open your_file.xlsx using pandas. Use UTF-8 encoding to support special characters
file = pd.read_csv('output.csv')

# Read all questions from the first column called Questions
questions = file['text']

def ask_gpt(question):
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4-turbo-preview",
            messages=[
                {
                    "role": "system",
                    "content": "1. Description\n\nThis file contains offensive and hateful content. We have collected tweets about the 2020 US presidential campaigns and election. Given the text of a tweet, we want to annotate the stance the tweet holds towards our predetermined targets and the presence or lack of hateful and offensive speech. Your output should consist of several different classifications of the text separated by commas. The classifications are Stance Detection, Hate Speech Detection, Discrimination Type, Severity, and Directedness. So your final output for each cell should look like this: [Stance Detection], [Hate Speech Detection], [Discrimination Type], [Severity], [Directedness]. No additional words should be added and you must classify each cell based on the categories I give. I’ll now go through each classification one by one and how I want you to determine which category to use.\n\n1.1 Stance Detection\n\nThis category identifies the stance of a tweet's text towards specific targets: Donald Trump, Joe Biden, and Kanye West. The stance is classified according to the tweet's support or opposition to these individuals, based on predetermined annotation labels:\n\nFavor: The tweet supports or advocates for the target.\nExample: \"Biden has the experience we need in a president.\"\nAgainst: The tweet opposes or criticizes the target.\nExample: \"Trump's policies have failed us.\"\nNeither: The target is not mentioned, either implicitly or explicitly.\nExample: \"Voting is our civic duty.\"\nMixed: The tweet contains both positive and negative aspects about the target.\nExample: \"Kanye is innovative but unpredictable.\"\nNeutral mentions: The tweet states facts or quotes without taking a position towards the target.\nExample: \"Trump signed a new executive order today.\"\n\nGuidelines:\n\nTweets must explicitly or implicitly mention the targets to be labeled under any category other than \"Neither.\"\nReferences to political parties or slogans associated with the targets can indicate a tweet's stance.\nExplicit mentions include direct names or titles, while implicit mentions may involve slogans or references to the candidates' positions.\nThe stance towards one target can be affected by mentions of associated parties or vice presidential candidates.\nOffensive or derogatory nicknames and hashtags that are critical of or supportive towards the targets or their parties should be considered in determining the tweet's stance.\n\n1.2 Hate Speech Detection\n\nThis category identifies tweets that contain hate speech or offensive language directed at individuals or groups, particularly in the context of the 2020 presidential election. Tweets are classified as either \"Hateful\" or \"Non-Hateful.\"\n\nHateful: Tweets that explicitly or implicitly demean or threaten a person or group based on aspects of their identity or political affiliations.\nExample: \"Candidate X's supporters are ruining our country.\"\nNon-Hateful: Tweets that do not contain language that demeans or threatens individuals or groups.\nExample: \"I disagree with Candidate Y's policies.\"\n\nGuidelines:\nHate speech can include abusive, degrading speech, violent threats, insults, and racial or sexist slurs.\nName-calling or derogatory references to political figures or their supporters is considered hateful.\nThe context and combination of words are crucial in determining whether speech is hateful or offensive.\nWhen determining if a tweet is hateful, consider whether the language used would be offensive to the average person.\nSlang and abbreviations commonly understood to be derogatory should be flagged as hateful, though some words may have context-dependent meanings.\n\n1.3 Discrimination Type\n\nThis category identifies the specific nature of discrimination present in the content. Classification is based on the targeted aspect of identity or characteristic. Each tweet should be evaluated for content that discriminates against individuals or groups based on the following types:\n\nN/A\nGeneral: Discriminatory content that does not fit into the more specific categories listed below but still contains elements of discrimination.\nExample: \"All politicians are corrupt and cannot be trusted.\"\nSexist: Content that discriminates based on gender, typically against women, including stereotypes or derogatory comments.\nExample: \"Women shouldn't be in politics; they're too emotional.\"\nSexual Harassment: Content that includes unwelcome sexual advances, requests for sexual favors, and other verbal or physical harassment of a sexual nature.\nExample: \"She only got her position by sleeping her way to the top.\"\nHomophobic: Content that expresses fear, hatred, discomfort with, or mistrust of people who are lesbian, gay, or bisexual.\nExample: \"Being gay is not natural and should not be promoted by politicians.\"\nRacist: Content that discriminates against individuals based on their race or ethnicity.\nExample: \"This country doesn't need a president from that race.\"\nTransphobic: Content that expresses fear, hatred, discomfort with, or mistrust of transgender individuals or transsexuality.\nExample: \"Transgender people shouldn't be allowed in the military.\"\nAbleist: Content that discriminates against people with disabilities.\nExample: \"He's too mentally unstable to hold office.\"\nIntellectual: Content that discriminates against individuals based on their intellectual capabilities or expressions of intellectual elitism.\nExample: \"Only educated people should be allowed to vote.\"\nAgeism: Content that discriminates on people based on age, referencing senility or declining mental faculties.\nExample: “He couldn't even remember his speech!”\n\nGuidelines:\n\nIdentify the Nature of Discrimination: Carefully assess each tweet to determine the specific type of discrimination it contains. Pay attention to the aspect of identity or characteristic that is being targeted.\nUse Examples for Clarity: Refer to provided examples to better understand the nuances of each discrimination type. This will aid in accurately classifying tweets that may not explicitly mention a category but imply discrimination.\nConsider Context: Sometimes, discrimination might be implicit or embedded in jokes, sarcasm, or cultural references. Consider the broader context of the tweet to accurately identify the type of discrimination.\n\n1.4 Severity\n\nThe severity of the detected hate speech or discrimination can be rated on a scale from 0 to 4, where each level represents the intensity of harm or potential for harm.\n\nN/A\n1: Mild - Content contains minimal discriminatory language or implications without intent to harm.\nExample: \"This candidate seems clueless about real-world problems.\"\n2: Moderate - Content explicitly discriminates or uses hate speech but falls short of inciting violence or harm.\nExample: \"Politicians like her are what's wrong with this country, always lying.\"\n3: Severe - Content contains clear, targeted hate speech or discrimination with potential to incite or endorse harm.\nExample: \"People from that place are destroying our society and need to be stopped.\"\n4: Extreme - Content explicitly incites violence, harm, or severe discrimination against individuals or groups.\nExample: \"All supporters of this candidate deserve to be punished.\"\n\nGuidelines:\n\nAssess Harm Potential: Evaluate the content's potential to cause harm or incite violence against individuals or groups. This involves understanding the intensity and impact of the language used.\nScale Appropriately: Use the provided scale from 0 to 4 to rate the severity of hate speech or discrimination. Ensure that the rating reflects both the explicitness and the potential harm of the content.\nAcknowledge Subjectivity: Recognize that assessing severity can be subjective. When in doubt, consider consulting with peers or supervisors to reach a consensus on the appropriate severity rating.\n\n1.5 Directedness\n\nThis category assesses whether the hate speech or discriminatory language is directed towards a specific individual or group, or if it's implicit or general in nature.\n\nN/A\nExplicit: The content clearly identifies an individual or group as the target of hate speech or discrimination. This can include direct mentions, tags, or clear references to specific identities.\nExample: \"John Doe is a disgrace to his country.\"\nImplicit: The content suggests hate speech or discrimination without directly naming or identifying the target. This can include veiled references, stereotypes, or coded language that implies a target without explicit mention.\nExample: \"Some people just don't belong in positions of power, especially if they can't understand basic values.\"\n\nGuidelines: \n\nDetermine Targetedness: Identify whether the hate speech or discrimination is directed towards specific individuals or groups, or if it is more implicit or general in nature.\nExplicit vs. Implicit: Distinguish between content that explicitly names or identifies its target and content that implies targets through veiled references or coded language.\nUse Judgment: Evaluating directedness may require judgment calls, especially with implicit content. Consider the potential targets and the context within which the statement is made to accurately classify the directedness of the content.\n\n1.6 Recap\n\nRemember: \nReason in order, step-by-step\nOnly pick one category for each classification\nIf something isn’t hateful be consistent and respond with N/A to 1.3 1.4 and 1.5\nYour final output for each cell should look like this: [Stance Detection], [Hate Speech Detection], [Discrimination Type], [Severity], [Directedness]\n"
    },
                {
                    "role": "user",
                    "content": question
                }
            ],
            temperature=0,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0
        )
        return response.choices[0].message.content
    except Exception as e:
        print("Error processing question:", question)
        print(traceback.format_exc())  # Print the error message
        return "Error"  # You could return None or "Error" to indicate a failed processing

# Initialize an empty list to store answers
answers = []

# Process each question
for index, question in enumerate(questions):
    answer = ask_gpt(question)
    answers.append(answer)
    
    # Print progress
    print(f"Progress: {index + 1}/{len(questions)}")
    
    # Save progress periodically, for example, after every 10 questions
    if (index + 1) % 10 == 0 or (index + 1) == len(questions):
        # Temporarily extend the DataFrame to match the length of answers
        temp_df = pd.DataFrame({'Model': answers})
        temp_df.to_csv('final_output_progress.csv', index=False)
        print("Saved progress")

# Once all processing is complete, ensure the final DataFrame matches the length of all answers
final_df = pd.DataFrame({'Model': answers})
final_df.to_csv('final_output.csv', index=False)
print("All questions processed and saved.")
